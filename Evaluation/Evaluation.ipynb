{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQsRjO_MeY0"
      },
      "source": [
        "The following files are needed for this notebook:\n",
        "\n",
        "\n",
        "1.   db/train.db\n",
        "2.   db/test.db\n",
        "3.   db/bad_indices.csv\n",
        "\n",
        "Please change the paths in the cells where ever these files are referenced to your drive path. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwWq32vI7EV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esxh-ENM8LfR"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GryfBNzx8KCe"
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m8WXh-qm3zY"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAapIJAOm1JZ"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikisql\")\n",
        "print(f\"Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQXoO77xJ_Yj"
      },
      "source": [
        "# Test the queries on train.db to check db connection\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/db/train.db') # change this to your drive path\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cur.fetchall())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S9J7bEVnP-4"
      },
      "source": [
        "# Function for evaluation\n",
        "import sqlite3\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def is_number(s):\n",
        "    try:\n",
        "        if float(s) or int(s):\n",
        "          return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def evaluate(path_to_db: str, test_data:list, predictions:list, bad_indices:list):\n",
        "  '''\n",
        "  path_to_db - The path in the filesystem which has the db file. Example: /content/drive/MyDrive/db/test.db\n",
        "  test_data - The test data list. This is the list contained in dataset['test']. This should contain all of the original keys.\n",
        "  predictions - The list containing predictions by the model. The indices shoould match with the test data\n",
        "  bad_indices - List of indices in the test data which are to be skipped\n",
        "  '''\n",
        "  try:\n",
        "    conn = sqlite3.connect(path_to_db)\n",
        "  except Error as e:\n",
        "    print(f'Error connecting to db: {e}')\n",
        "  cur = conn.cursor()\n",
        "  count = 0\n",
        "  bad_index = []\n",
        "  for k, sample in enumerate(test_data):\n",
        "    if k in bad_indices:\n",
        "      continue\n",
        "    ground_truth = sample['sql']['human_readable']\n",
        "    prediction = predictions[k]\n",
        "    parsed_gt = parse_query(ground_truth, column_index=sample['sql']['conds']['column_index'], \n",
        "                            column_names=sample['table']['header'],\n",
        "                            conditions=sample['sql']['conds']['condition'],\n",
        "                            agg=sample['sql']['agg'],\n",
        "                            table_id=sample['table']['id'])\n",
        "    parsed_prediction = parse_query(prediction, column_index=sample['sql']['conds']['column_index'], \n",
        "                            column_names=sample['table']['header'],\n",
        "                            conditions=sample['sql']['conds']['condition'],\n",
        "                            agg=sample['sql']['agg'],\n",
        "                            table_id=sample['table']['id'])\n",
        "    try:\n",
        "      cur.execute(parsed_gt)\n",
        "      gt_rows = cur.fetchall()\n",
        "    except:\n",
        "      continue\n",
        "    try:\n",
        "      cur.execute(parsed_prediction)\n",
        "      predicted_rows = cur.fetchall()\n",
        "      if len(gt_rows) == len(predicted_rows):\n",
        "        count += 1\n",
        "    except:\n",
        "      e = sys.exc_info()[0]\n",
        "      print(f\"Execution failed for index {k}, GT: {ground_truth}, Predicted: {prediction}\")\n",
        "  eval_score = count / (len(test_data)-len(bad_indices))\n",
        "  return eval_score\n",
        "\n",
        "def parse_query(query:str, column_index:list, column_names:list, conditions:list, agg:int, table_id:str):\n",
        "  '''\n",
        "  query - the query to be parsed\n",
        "  column_index - data['sql']['conds']['column_index']\n",
        "  column_names - data['table']['header']\n",
        "  conditions - data['sql']['conds']['condition']\n",
        "  agg - data['sql']['agg']\n",
        "  table_id - data['table']['id']\n",
        "  '''\n",
        "  agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
        "  query = query.replace('\"', '')\n",
        "  table_name = 'table_'+table_id.replace('-','_')\n",
        "  column_to_index = {}\n",
        "  for i, column in enumerate(column_names):\n",
        "    column_to_index[column] = str(i)\n",
        "  sorted_column_to_index = {}\n",
        "  for i in sorted(column_to_index, key=len, reverse=True):\n",
        "      sorted_column_to_index[i] = column_to_index[i]\n",
        "  column_to_index = sorted_column_to_index\n",
        "  # print(column_to_index)\n",
        "  seen = []\n",
        "  for condition in conditions:\n",
        "    # print(f'Condition: {condition}, Query: {query}')\n",
        "    if condition in seen:\n",
        "      continue\n",
        "    condition = condition.replace('\"', '')\n",
        "    if not is_number(condition):\n",
        "      query = query.replace(condition, '\"'+condition+'\"')\n",
        "      # query = re.sub(r\"\\b%s\\b\" % condition , '\"'+condition+'\"', query)\n",
        "    seen.append(condition)\n",
        "  for column, column_index in column_to_index.items():\n",
        "    query = query.replace(column, 'col'+column_index)\n",
        "    # query = re.sub(r\"\\b%s\\b\" % column, 'col'+column_index, query)\n",
        "  query = query.replace('table', table_name)\n",
        "  agg_op = agg_ops[agg]\n",
        "  if agg != 0:\n",
        "    idx = query.find(agg_op)\n",
        "    start = idx + len(agg_op) + 1\n",
        "    end = start\n",
        "    while end < len(query) and query[end] != ' ':\n",
        "      end += 1\n",
        "    agg_columns = query[start:end]\n",
        "    query = query.replace(query[idx:end], agg_op+'('+agg_columns+')')\n",
        "  return query\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VELX3T-R2GAH"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# change this to your drive path\n",
        "with open('/content/drive/MyDrive/db/bad_indices.csv', newline='') as f: \n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    bad_indices = [int(row[1])for row in reader if row]\n",
        "print(len(bad_indices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYezKkU6DL61"
      },
      "source": [
        "test_data = dataset['test']\n",
        "path_to_db = '/content/drive/MyDrive/db/test.db' # change this to your drive path"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLYOl-D86n4R"
      },
      "source": [
        "from simpletransformers.seq2seq import Seq2SeqModel\n",
        "\n",
        "# use this for BART models:\n",
        "model = Seq2SeqModel(\n",
        "    encoder_decoder_type='bart',\n",
        "    encoder_decoder_name=\"/content/drive/MyDrive/cs685_models/BART-INTER-FINE-TUNED/final_outputs_bart_int_50\", #add your trained model path here\n",
        "    use_cuda=True,\n",
        ")\n",
        "\n",
        "# use this for other models:\n",
        "# model = Seq2SeqModel(\n",
        "#     encoder_type='distilbert',\n",
        "#     encoder_name=\"/content/drive/MyDrive/cs685_models/DISTILBERT-BERT/final_outputs_distilbert/encoder\",\n",
        "#     decoder_name=\"/content/drive/MyDrive/cs685_models/DISTILBERT-BERT/final_outputs_distilbert/decoder\",\n",
        "#     use_cuda=True,\n",
        "# )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_UINCOT8VFV"
      },
      "source": [
        "# Get the predictions from the loaded model\n",
        "\n",
        "queries = []\n",
        "for i, sample in enumerate(dataset['test']):\n",
        "  queries.append(sample['sql']['human_readable'])\n",
        "predictions = []\n",
        "predictions = model.predict(queries)\n",
        "assert len(predictions) == len(queries)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYDTT7HcS-qe"
      },
      "source": [
        "# A sanity check for the predictions\n",
        "\n",
        "import random\n",
        "index = random.randint(0,15000)\n",
        "print(f\"GT: {queries[index]}, Prediction: {predictions[index]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YqOIrxeDYEl"
      },
      "source": [
        "# Get the execution score\n",
        "\n",
        "eval_score = evaluate(path_to_db, test_data, predictions, bad_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9FVeXALK6zq"
      },
      "source": [
        "print(f\"Score: {eval_score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPT5AnmgNLgB"
      },
      "source": [
        "# Check if the ground truth and predicted queries match\n",
        "\n",
        "def compare_queries(queries:str, predictions:str):\n",
        "  if len(queries) != len(predictions):\n",
        "    raise ValueError(\"Lengths don't match\")\n",
        "  score = 0.0\n",
        "  for i in range(len(queries)):\n",
        "    if queries[i] == predictions[i]:\n",
        "        score += 1\n",
        "  score = score / len(queries)\n",
        "  return score\n",
        "score = compare_queries(queries, predictions)\n",
        "print(f\"Score: {score}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8xMw7_8kd2U"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwRYhA705ldt"
      },
      "source": [
        "The following cells will install some of the libraries being used for this project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru9H7ZZlrOaI"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZpl5J3YEg3I"
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKIkKMIiEjvp"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hBpYW6J5sbV"
      },
      "source": [
        "We will now load the WikiSQL dataset, which is provided out of the box by the HuggingFace library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IiC7NluEm7S",
        "outputId": "e0207ded-91dd-4a88-92d7-b5dafdf3bc7e"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikisql\")\n",
        "print(dataset['train'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset wiki_sql (/root/.cache/huggingface/datasets/wiki_sql/default/0.1.0/2e98053891fd8f9b2c4348bba609ce40cc0a4d7f621191cebcd7cb558b5f8a70)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'phase': 1, 'question': 'Tell me what the notes are for South Australia ', 'sql': {'agg': 0, 'conds': {'column_index': [3], 'condition': ['SOUTH AUSTRALIA'], 'operator_index': [0]}, 'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA', 'sel': 5}, 'table': {'caption': '', 'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'id': '1-1000181-1', 'name': 'table_1000181_1', 'page_id': '', 'page_title': '', 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'section_title': '', 'types': ['text', 'text', 'text', 'text', 'text', 'text']}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcQusIqS51yF"
      },
      "source": [
        "Some data preprocessing, which is done to make the data suitable for training. We extract the ground truth questions and queries from the train and validation splits and store them in a list, which is then converted to a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwsdEka8Jbjl"
      },
      "source": [
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "from simpletransformers.seq2seq import Seq2SeqModel\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "train = []\n",
        "for d in dataset[\"train\"]:\n",
        "  temp = []\n",
        "  temp.append(d['question'])\n",
        "  temp.append(d[\"sql\"][\"human_readable\"])\n",
        "  train.append(temp)\n",
        "train_df = pd.DataFrame(train, columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "validation = []\n",
        "for d in dataset[\"validation\"]:\n",
        "  temp = []\n",
        "  temp.append(d['question'])\n",
        "  temp.append(d[\"sql\"][\"human_readable\"])\n",
        "  validation.append(temp)\n",
        "eval_df = pd.DataFrame(validation, columns=[\"input_text\", \"target_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnRGzUyY6m9x"
      },
      "source": [
        "We will now define the model training arguments and the model itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CodCpIDnE4DU"
      },
      "source": [
        "model_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"max_seq_length\": 50,\n",
        "    \"train_batch_size\": 64,\n",
        "    \"num_train_epochs\": 20,\n",
        "    \"save_eval_checkpoints\": False,\n",
        "    \"save_model_every_epoch\": False,\n",
        "    \"evaluate_generated_text\": True,\n",
        "    \"evaluate_during_training_verbose\": True,\n",
        "    \"use_multiprocessing\": False,\n",
        "    \"max_length\": 25,\n",
        "    \"manual_seed\": 4,\n",
        "    \"weight_decay\":0.01,\n",
        "    \"learning_rate\":8e-5,\n",
        "    \"warmup_steps\":1000,\n",
        "    \"output_dir\":\"/content/drive/MyDrive/cs685_models/BART1\"\n",
        "}\n",
        "\n",
        "model = Seq2SeqModel(\n",
        "    encoder_decoder_type='bart',\n",
        "    encoder_decoder_name='facebook/bart-base'\n",
        "    args=model_args,\n",
        "    use_cuda=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDMiUUyV6vzB"
      },
      "source": [
        "We can now start training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-vyVnh_E_Oy"
      },
      "source": [
        "model.train_model(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copXC9oF64gq"
      },
      "source": [
        "We now run the evaluation on the validation dataset. The validation loss is used for tuning the hyperparametrs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH4GtJrHRoh2"
      },
      "source": [
        "results = model.eval_model(eval_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}